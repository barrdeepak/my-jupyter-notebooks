{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309190e0-0806-4620-acb3-5353a7d2c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "class IcebergUtility:\n",
    "    MAX_ROW_DISPLAY = 100\n",
    "\n",
    "    @staticmethod\n",
    "    def list_snapshot_ids(table_name: str, spark: SparkSession) -> list[int]:\n",
    "        catalog_name, database_name, tbl_name = table_name.split(\".\")\n",
    "        jvm = spark._jvm\n",
    "        conf = jvm.org.apache.hadoop.conf.Configuration()\n",
    "        # catalog = spark._jsparkSession.sessionState().catalogManager().catalog(catalog_name)\n",
    "        catalog = jvm.org.apache.iceberg.hadoop.HadoopCatalog(conf, \"file:///home/jovyan/work/iceberg/warehouse\")\n",
    "        # iceberg_catalog = catalog.icebergCatalog()\n",
    "        # table_identifier = iceberg_catalog.TableIdentifier.of(database_name, tbl_name)\n",
    "        table_identifier = jvm.org.apache.iceberg.catalog.TableIdentifier.parse(\"db.fruits_price\")\n",
    "        table = catalog.loadTable(table_identifier)\n",
    "        \n",
    "        snapshots = list(table.snapshots())\n",
    "\n",
    "        for snapshot in snapshots:\n",
    "            print(f\"Snapshot ID: {snapshot.snapshotId()}\")\n",
    "            print(f\"Timestamp: {snapshot.timestampMillis()}\")\n",
    "            print(f\"Operation: {snapshot.operation()}\")\n",
    "            print(f\"Summary: {snapshot.summary()}\")\n",
    "            print(\"--------\")\n",
    "\n",
    "        print(f\"Total snapshots = {len(snapshots)}\")\n",
    "        return [snapshot.snapshotId() for snapshot in snapshots]\n",
    "\n",
    "    @staticmethod\n",
    "    def show_table_contents(table_name: str, spark: SparkSession, snapshot_id: int = None):\n",
    "        if snapshot_id:\n",
    "            df = spark.read.format(\"iceberg\") \\\n",
    "                .option(\"snapshot-id\", snapshot_id) \\\n",
    "                .load(table_name)\n",
    "        else:\n",
    "            df = spark.read.format(\"iceberg\").load(table_name)\n",
    "        \n",
    "        df.show(IcebergUtility.MAX_ROW_DISPLAY)\n",
    "\n",
    "    @staticmethod\n",
    "    def write_as_table(df, table_name: str):\n",
    "        df.writeTo(table_name).using(\"iceberg\").createOrReplace()\n",
    "\n",
    "    @staticmethod\n",
    "    def write_as_partitioned_table(df, table_name: str, partition_col: str):\n",
    "        df.writeTo(table_name).partitionedBy(col(partition_col)).createOrReplace()\n",
    "\n",
    "    @staticmethod\n",
    "    def append_to_table(df, table_name: str):\n",
    "        df.writeTo(table_name).append()\n",
    "\n",
    "    @staticmethod\n",
    "    def count_rows(table_name: str, spark: SparkSession) -> int:\n",
    "        df = spark.read.format(\"iceberg\").load(table_name)\n",
    "        return df.count()\n",
    "\n",
    "    @staticmethod\n",
    "    def describe_table(table_name: str, spark: SparkSession):\n",
    "        spark.sql(f\"DESCRIBE TABLE {table_name}\").show(truncate=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def list_tables(catalog: str, namespace: str, spark: SparkSession):\n",
    "        tables = spark.catalog.listTables(f\"{catalog}.{namespace}\")\n",
    "        print(f\" Number of tables : {len(tables)}\\n\")\n",
    "        for t in tables:\n",
    "            print(f\"[ {t.catalog} | {t.namespace} | {t.name} ]\")\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def delete_table(table_name: str, spark: SparkSession):\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_name} PURGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc92498b-986c-4856-8167-89a05845a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, round as pyspark_round\n",
    "\n",
    "class Utility:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_spark_session() -> SparkSession:\n",
    "        spark = (\n",
    "            SparkSession.builder\n",
    "            .appName(\"Iceberg demo app\")\n",
    "            .master(\"local[*]\")  # use local mode\n",
    "            .config(\"spark.sql.catalog.my_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "            .config(\"spark.sql.catalog.my_catalog.type\", \"hadoop\")\n",
    "            .config(\"spark.sql.catalog.my_catalog.warehouse\", \"warehouse\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "        return spark\n",
    "\n",
    "    @staticmethod\n",
    "    def read_file(file: str, spark: SparkSession):\n",
    "        df = spark.read.option(\"multiline\", \"true\").json(file)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_column(df, col_name: str):\n",
    "        return df.drop(col_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_column(df, col_name: str, value):\n",
    "        return df.withColumn(col_name, lit(value))\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_discount(df):\n",
    "        return df.withColumn(\n",
    "            \"final_price\",\n",
    "            pyspark_round(col(\"price\") * (lit(1) - col(\"discount\") / 100.0), 3)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77daec2-c33e-40ca-9b78-ce46bf839176",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"input/fruits.json\"\n",
    "output_table = \"my_catalog.db.fruits_price\"\n",
    "partition_column = \"date\"\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = Utility.get_spark_session()\n",
    "\n",
    "# Delete the table if it exists\n",
    "IcebergUtility.delete_table(output_table, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f799b3-5a8f-45cc-a946-46cbcbc56466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and transform the data - Apply 5% Discount\n",
    "df = Utility.read_file(input_file, spark)\n",
    "df = Utility.drop_column(df, \"discount\")\n",
    "df = Utility.add_column(df, \"discount\", 5.0)\n",
    "df = Utility.add_column(df, \"date\", \"2025-06-14\")\n",
    "df = Utility.apply_discount(df)\n",
    "\n",
    "# Write the DataFrame as an Iceberg table\n",
    "IcebergUtility.write_as_partitioned_table(df, output_table, partition_column)\n",
    "\n",
    "\n",
    "# Display the table contents\n",
    "IcebergUtility.show_table_contents(output_table, spark)\n",
    "count = IcebergUtility.count_rows(output_table,spark)\n",
    "print(f\"Number of rows : {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad44ba-c414-4bbf-b85d-9e3d284ed79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IcebergUtility.list_tables(\"my_catalog\",\"db\", spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43aa447-ef4a-4d2b-a952-a9a722f90497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and transform the data - Apply 10% Discount\n",
    "df = Utility.read_file(input_file, spark)\n",
    "df = Utility.drop_column(df, \"discount\")\n",
    "df = Utility.add_column(df, \"discount\", 10.0)\n",
    "df = Utility.add_column(df, \"date\", \"2025-06-15\")\n",
    "df = Utility.apply_discount(df)\n",
    "\n",
    "# Write the DataFrame as an Iceberg table\n",
    "IcebergUtility.append_to_table(df, output_table)\n",
    "\n",
    "# Display the table contents\n",
    "IcebergUtility.show_table_contents(output_table, spark)\n",
    "count = IcebergUtility.count_rows(output_table,spark)\n",
    "print(f\"Number of rows : {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4194f-7103-49dc-8a94-f86541b82c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and transform the data - Apply 15% Discount\n",
    "df = Utility.read_file(input_file, spark)\n",
    "df = Utility.drop_column(df, \"discount\")\n",
    "df = Utility.add_column(df, \"discount\", 15.0)\n",
    "df = Utility.add_column(df, \"date\", \"2025-06-16\")\n",
    "df = Utility.apply_discount(df)\n",
    "\n",
    "# Write the DataFrame as an Iceberg table\n",
    "IcebergUtility.append_to_table(df, output_table)\n",
    "\n",
    "# Display the table contents\n",
    "IcebergUtility.show_table_contents(output_table, spark)\n",
    "count = IcebergUtility.count_rows(output_table,spark)\n",
    "print(f\"Number of rows : {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08784a2b-eed5-4ec0-9dbb-98a6622e6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = IcebergUtility.list_snapshot_ids(output_table, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2f79e-6ae3-4037-9861-527f947fa186",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in snapshots:\n",
    "    print(f\"Snapshot Id : {id}\")\n",
    "    IcebergUtility.show_table_contents(output_table, spark, id)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
