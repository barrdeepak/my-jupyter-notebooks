{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309190e0-0806-4620-acb3-5353a7d2c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "class IcebergUtility:\n",
    "    MAX_ROW_DISPLAY = 100\n",
    "\n",
    "    @staticmethod\n",
    "    def list_snapshot_ids(table_name: str, spark: SparkSession) -> list[int]:\n",
    "        catalog_name, database_name, tbl_name = table_name.split(\".\")\n",
    "        jvm = spark._jvm\n",
    "        conf = jvm.org.apache.hadoop.conf.Configuration()\n",
    "        # catalog = spark._jsparkSession.sessionState().catalogManager().catalog(catalog_name)\n",
    "        catalog = jvm.org.apache.iceberg.hadoop.HadoopCatalog(conf, \"file:///home/jovyan/work/iceberg/warehouse\")\n",
    "        # iceberg_catalog = catalog.icebergCatalog()\n",
    "        # table_identifier = iceberg_catalog.TableIdentifier.of(database_name, tbl_name)\n",
    "        table_identifier = jvm.org.apache.iceberg.catalog.TableIdentifier.parse(\"db.fruits_price\")\n",
    "        table = catalog.loadTable(table_identifier)\n",
    "        \n",
    "        snapshots = list(table.snapshots())\n",
    "\n",
    "        for snapshot in snapshots:\n",
    "            print(f\"Snapshot ID: {snapshot.snapshotId()}\")\n",
    "            print(f\"Timestamp: {snapshot.timestampMillis()}\")\n",
    "            print(f\"Operation: {snapshot.operation()}\")\n",
    "            print(f\"Summary: {snapshot.summary()}\")\n",
    "            print(\"--------\")\n",
    "\n",
    "        print(f\"Total snapshots = {len(snapshots)}\")\n",
    "        return [snapshot.snapshotId() for snapshot in snapshots]\n",
    "\n",
    "    @staticmethod\n",
    "    def show_table_contents(table_name: str, spark: SparkSession, snapshot_id: int = None):\n",
    "        if snapshot_id:\n",
    "            df = spark.read.format(\"iceberg\") \\\n",
    "                .option(\"snapshot-id\", snapshot_id) \\\n",
    "                .load(table_name)\n",
    "        else:\n",
    "            df = spark.read.format(\"iceberg\").load(table_name)\n",
    "        df.show(IcebergUtility.MAX_ROW_DISPLAY)\n",
    "        \n",
    "    @staticmethod\n",
    "    def write_as_table(df, table_name: str, file_fmt:str = None):\n",
    "        if file_fmt is not None and file_fmt.lower() =='orc':\n",
    "            df.writeTo(table_name).using(\"iceberg\").tableProperty(\"write.format.default\", file_fmt).createOrReplace()\n",
    "        else:\n",
    "            df.writeTo(table_name).using(\"iceberg\").createOrReplace()\n",
    "\n",
    "    @staticmethod\n",
    "    def write_as_partitioned_table(df, table_name: str, partition_col: str, file_fmt:str = None):\n",
    "        if file_fmt is not None and file_fmt.lower() =='orc':\n",
    "            df.writeTo(table_name).partitionedBy(col(partition_col)).using(\"iceberg\").tableProperty(\"write.format.default\", file_fmt).createOrReplace()\n",
    "        else:\n",
    "            df.writeTo(table_name).partitionedBy(col(partition_col)).using(\"iceberg\").createOrReplace()\n",
    "\n",
    "    @staticmethod\n",
    "    def write_as_table_with_bloom(df, table_name: str, bloom_filter_column: str, bloom_filter_max_items: str):\n",
    "        df.writeTo(table_name).using(\"iceberg\").tableProperty(\"write.metadata.bloom-filter.columns\", bloom_filter_column).tableProperty(\"write.metadata.bloom-filter.max-num-items\", bloom_filter_max_items).createOrReplace()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def append_to_table(df, table_name: str):\n",
    "        df.writeTo(table_name).append()\n",
    "\n",
    "    @staticmethod\n",
    "    def count_rows(table_name: str, spark: SparkSession) -> int:\n",
    "        df = spark.read.format(\"iceberg\").load(table_name)\n",
    "        return df.count()\n",
    "\n",
    "    @staticmethod\n",
    "    def describe_table(table_name: str, spark: SparkSession):\n",
    "        spark.sql(f\"DESCRIBE TABLE {table_name}\").show(truncate=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def list_tables(catalog: str, namespace: str, spark: SparkSession):\n",
    "        tables = spark.catalog.listTables(f\"{catalog}.{namespace}\")\n",
    "        print(f\" Number of tables : {len(tables)}\\n\")\n",
    "        for t in tables:\n",
    "            print(f\"[ {t.catalog} | {t.namespace} | {t.name} ]\")\n",
    "\n",
    "    @staticmethod\n",
    "    def delete_table(table_name: str, spark: SparkSession):\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_name} PURGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc92498b-986c-4856-8167-89a05845a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, round as pyspark_round\n",
    "\n",
    "class Utility:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_spark_session() -> SparkSession:\n",
    "        spark = (\n",
    "            SparkSession.builder\n",
    "            .appName(\"Iceberg demo app\")\n",
    "            .master(\"local[*]\")  # use local mode\n",
    "            .config(\"spark.sql.catalog.my_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "            .config(\"spark.sql.catalog.my_catalog.type\", \"hadoop\")\n",
    "            .config(\"spark.sql.catalog.my_catalog.warehouse\", \"warehouse\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "        return spark\n",
    "\n",
    "    @staticmethod\n",
    "    def read_file(file: str, spark: SparkSession):\n",
    "        df = spark.read.option(\"multiline\", \"true\").json(file)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_column(df, col_name: str):\n",
    "        return df.drop(col_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_column(df, col_name: str, value):\n",
    "        return df.withColumn(col_name, lit(value))\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_discount(df):\n",
    "        return df.withColumn(\n",
    "            \"final_price\",\n",
    "            pyspark_round(col(\"price\") * (lit(1) - col(\"discount\") / 100.0), 3)\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
